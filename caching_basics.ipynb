{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dadc7f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chose one or the other:\n",
    "# from openai import AsyncOpenAI\n",
    "from langfuse.openai import AsyncOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfb0d6b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dce0b7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "What do you call a fake noodle?\n",
      "\n",
      "(wait for it...)\n",
      "\n",
      "An impasta!\n",
      "\n",
      "I hope that made you smile! Do you want to hear another one?\n"
     ]
    }
   ],
   "source": [
    "client = AsyncOpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",  # Ollama's default local endpoint\n",
    "    api_key=\"ollama\"  # Dummy key, required but not used by Ollama\n",
    ")\n",
    "\n",
    "def _msg(role, content):\n",
    "    # simple helper function to create a message object\n",
    "    return {\"role\": role, \"content\": content}\n",
    "\n",
    "def system(content):\n",
    "    return _msg(\"system\", content)\n",
    "\n",
    "def user(content):\n",
    "    return _msg(\"user\", content)\n",
    "\n",
    "def assistant(content):\n",
    "    return _msg(\"assistant\", content)\n",
    "\n",
    "# sanity check:\n",
    "completion = await client.chat.completions.create(\n",
    "    messages=[system(\"You are a helpful assistant.\"), user(\"Tell me a joke\")],\n",
    "    model=\"llama3.2:3b\",\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478816ad",
   "metadata": {},
   "source": [
    "# Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f84f5f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diskcache_decorator import cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be37fa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache = Cache() # temporary cache (as long as the kernel is running)\n",
    "# cache = Cache(directory=\".cache\") # persistent cache\n",
    "\n",
    "# async def set_async(key, value, **kwargs):\n",
    "#     return await asyncio.to_thread(cache.set(key, value, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bc40927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_to_disk(_func=None, *, cache_path='.cache', **kwargs):\n",
    "    \"\"\"Decorator that caches to .cache directory.\n",
    "    Can be used as @cache_to_disk or @cache_to_disk(...).\n",
    "    Extra kwargs are forwarded to diskcache_decorator.cached.\n",
    "    \"\"\"\n",
    "    def _decorator(func):\n",
    "        return cached(cache_path=cache_path, **kwargs)(func)\n",
    "    return _decorator if _func is None else _decorator(_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a034876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cache_to_disk\n",
    "async def get_completion(model, messages):\n",
    "    return await client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=model,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81992a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The old bookstore creaked as the librarian pulled open the worn leather cover of the rarest novel in her collection.\n"
     ]
    }
   ],
   "source": [
    "completion = await get_completion(model=\"llama3.2:3b\", messages=[system(\"You are a helpful assistant.\"), user(\"Write a random sentence.\")])\n",
    "\n",
    "print(completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85968539",
   "metadata": {},
   "source": [
    "# Tracing\n",
    "\n",
    "(using LLM client and helper functions from caching part above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2a63042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langfuse.openai import AsyncOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "181dc960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run helper cell above for system client, user. ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89897f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(2):\n",
    "    completion = await client.chat.completions.create(\n",
    "        messages=[system(\"You are a helpful assistant.\"), user(\"Tell me a joke\")],\n",
    "        model=\"llama3.2:3b\",\n",
    "        temperature=0.5,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "729ca467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "What do you call a fake noodle?\n",
      "\n",
      "An impasta!\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "900dbf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing if caching does not show up in langfuse. witch earlier wrapper function:\n",
    "\n",
    "completion = await get_completion(model=\"llama3.2:3b\", messages=[system(\"You are a helpful assistant.\"), user(\"Write a random sentence. lololol\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1504411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The old bookstore creaked as the librarian pulled open the worn leather cover of the rarest novel in her collection.\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
